---
layout: post
title: 利用文本预测MBTI性格
category: 机器学习
catalog: true
mathjax: true
lang: ch
tags: 
    - 2017
    - Python
    - 机器学习
---

*Kaggle是个好地方，每次都能从里面发现很有意思的数据集。比如这次找到了一个叫[(MBTI) Myers-Briggs Personality Type Dataset ](https://www.kaggle.com/datasnaek/mbti-type) 的数据集，里面包含了8600个不同个人的MBTI 类型和这个人的posts。参考了kaggle 上几位大佬的做法[[1]](#1)，我用这个数据集训练出了一个基于文本的性格预测模型，最后总结出了马男波杰克剧中各个人物的MBTI 类型。根据[维基百科](https://www.wikiwand.com/zh-hans/%E9%82%81%E7%88%BE%E6%96%AF-%E5%B8%83%E9%87%8C%E6%A0%BC%E6%96%AF%E6%80%A7%E6%A0%BC%E5%88%86%E9%A1%9E%E6%B3%95)的说法，MBTI虽然在商界很流行，但其测试结果并不可靠，所以这个博客的结果仅供娱乐咯*

# 1. MBTI 模型

引用维基百科对MBTI的介绍如下：
>## 理论基础    
>此测验的性格分类源自荣格的主观观察，而非控制实验。荣格认为人的认知有四个部份，各有两个极端。MBTI测验就是依此理论为基础所发展出来的。     
>## 测试元素    
>这四个问题是：     
心理能力的走向：你是“外向”（Extrovert）（E）还是“内向”（Introvert）（I）？    
认识外在世界的方法：你是“实感”（Sensing）（S）还是“直觉”（Intuition）（N）？    
倚赖甚么方式做决定：你是“理性”（Thinking）（T）还是“情感”（Feeling）（F）？    
生活方式和处事态度：你是“判断”（Judging）（J）还是“理解”（Perceiving）（P）？   
根据4个问题的不同答案，可将人的性格分为16个种类。


这就比如说，一个内向，直觉，理性和倾向于理解的人会被标记为INTP。除此之外还有许多相关的内容可以根据标签对这个人的偏好或行为进行建模。从心理学的角度来看，它是基于Carl Jung（荣格类型学）的分类，并由8个不同的思维过程或思维方式构成的模型。虽然由于相关实验的不可靠性等原因使其可靠性受到了质疑。但是在很多领域它仍然是非常有用的。

# 2. 数据集分析

由于从Kaggle 得到数据都是已经标记好的，所以我们可以先看一看不同的性格类别所占的比例。结果如下：    
![](https://raw.githubusercontent.com/Donche/Donche.github.io/master/_posts/ML/mbti_num_ratio.png)    
由图可见数据集是非常不平衡的，例如I、N、P 的人数远大于E、S、J，这就会带来一些麻烦，有时会需要进行额外的处理[[2]](#2)。由于数据集是英文的，在处理上可以省去中文分析中所需要的分词步骤。   
接下来再看看不同性格特点常用词的情况。为了分析方便，这里我只将所有性格分为了了内向（Introvert）与外向（Extrovert）两种。生成词云用python 的wordcloud，简单快捷。为了去除一些信息量很少的词（如常用的介词、连词、代词等等...），需要将一些词从统计中去除掉，称为stopwords。在我们的处理中，除了英文本身的stopwords 之外，还需要去掉一些所有性格在posts 中都喜欢提到的词，比如 “think”、“people”、“thing” 等等。实现代码如下：
```python

# Data Aggregate
import numpy as np
import pandas as pd
import sklearn

data = pd.read_csv('mbti.csv')
type_quote = data.groupby('type').sum()
e_posts = ''
i_posts = ''
for _type in type_quote.index:
    if 'E' in _type:
        e_posts += type_quote.loc[_type].posts
    else:
        i_posts += type_quote.loc[_type].posts

# Generate wordcloud
from wordcloud import WordCloud, STOPWORDS

stopwords = set(STOPWORDS)
stopwords.add("think")
stopwords.add("people")
stopwords.add("thing")
my_wordcloud = WordCloud(width=800, height=800, stopwords=stopwords, background_color='white')
# Introvert
my_wordcloud_i = my_wordcloud.generate(i_posts)
plt.subplots(figsize = (15,15))
plt.imshow(my_wordcloud_infj)
plt.axis("off")
plt.title('Introvert', fontsize = 30)
plt.show()
#Extrovert
my_wordcloud_e = my_wordcloud.generate(e_posts)
plt.subplots(figsize = (15,15))
plt.imshow(my_wordcloud_infj)
plt.axis("off")
plt.title('Extrovert', fontsize = 30)
plt.show()
```

用这种方法分别做出了Introvert 和 Extrovert 的词云如下：   
![](https://raw.githubusercontent.com/Donche/Donche.github.io/master/_posts/ML//i_wordcloud.png)   
![](https://raw.githubusercontent.com/Donche/Donche.github.io/master/_posts/ML//e_wordcloud.png)   
唔...看到内向词云下方大大的youtube watch，我陷入了沉思...（但是看youtube 真的很有意思啊！）而外向的really know love也比内向的大很多，在使用频率上远超其他词语，与内向词云中相对均衡的分布形成很鲜明的对比。    

# 3. 文本向量化与模型训练
接下来我们要训练一个可以根据posts 对mbti 性格类型进行分类的模型。不过首先，需要熟悉一下文本向量化的一些方法。

## 3.1 文本向量化
### 3.1.1 词袋表征
词袋表征（The Bag of Words representation）是采用将文档文件转化为数值特征的一般过程的相关策略。将文档表示为向量的过程称为向量化。通过将每个词表示为一个整型，统计每个词出现的频率，可以将一个文档标识为一个向量。由于大多数文档只用到了所有可能用词的一个子集，因此文本向量化产生的矩阵大多为稀疏矩阵。

### 3.1.2 CountVectorizer

这是一个sklearn 内置的文本向量化函数，也是最基本的一个。向量的值为该词语出现频率。该函数有很多参数，但一般情况下取默认值即可。使用非常便捷：
```python
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(min_df=1)

corpus = [
     'This is the first document.',
     'This is the second second document.',
     'And the third one.',
     'Is this the first document?',
 ]
X = vectorizer.fit_transform(corpus)
X                              

<4x9 sparse matrix of type '<... 'numpy.int64'>'
    with 19 stored elements in Compressed Sparse Column format>
X.toarray()
array([[0, 1, 1, 1, 0, 0, 1, 0, 1],
       [0, 1, 0, 1, 0, 2, 1, 0, 1],
       [1, 0, 0, 0, 1, 0, 1, 1, 0],
       [0, 1, 1, 1, 0, 0, 1, 0, 1]]...)
```

### 3.1.3 Tf-idf
在一些文本语料库中，一些词非常常见（例如，英文中的“the”，“a”，“is”），因此很少带有文档实际内容的有用信息。如果我们将单纯的计数数据直接送给分类器，那些频繁出现的词会掩盖那些很少出现但是更有意义的词的频率。因此为了重新计算特征的计数权重，以便转化为适合分类器使用的浮点值，通常都会进行tf-idf转换。Tf代表词频（term-frequency），而tf-idf代表词频乘以逆向文档频率（inverse document-frequency）。即   
$${\displaystyle \mathrm {tfidf} (t,d,D)=\mathrm {tf} (t,d)\cdot \mathrm {idf} (t,D)}$$   
因为出现频率过高的词携带的信息并不多，所以这种方式有效增强了矩阵对文档的表示能力。但是这种方法在一些情况下依然没有countVectorizer 的binary occurrence markers 表现好，需要具体情况具体分析。在sklearn 中，TfidfVectorizer 将CountVectorizer 和TfidfTransformer 合并在一个模型中，使用也非常方便：   

```python
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(min_df=1)
vectorizer.fit_transform(corpus)
...                                
<4x9 sparse matrix of type '<... 'numpy.float64'>'
    with 19 stored elements in Compressed Sparse Row format>
```


## 3.2 不同分类模型性能对比
在这个数据集上，我发现使用countVectorizer 性能比Tf-idf还要好一些（kaggle 上的几个kernel 的结果也证明了这点）。然后我使用了不同的机器模型（Logistic Regression、Naive Bayes、Multilayer Perceptron、Extra-trees Classifier 和 Random Forest）分别训练，使用pipeline进行文本向量化与学习模型的串行化处理，其中Logitic Regression的代码大致如下：
```python
from sklearn import ensemble
from sklearn import feature_extraction
from sklearn import decomposition
from sklearn import pipeline
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_validate


kfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)
etc = ensemble.ExtraTreesClassifier(n_estimators = 20, max_depth=4, n_jobs = -1)
countV = feature_extraction.text.CountVectorizer(ngram_range=(1, 1), stop_words='english', lowercase = True,  max_features = 5000)
scoring = { 'neg_log_loss': 'neg_log_loss', 'f1_micro': 'f1_micro'}

# Logistic Regression
from sklearn.linear_model import LogisticRegression
model_lr = pipeline.Pipeline([('tfidf1', countV), ('lr', LogisticRegression(class_weight="balanced", C=0.005))])

results_lr = cross_validate(model_lr, data['posts'], data['type'], cv=kfolds, 
                          scoring=scoring, n_jobs=-1)

print("CV F1: {:0.4f} (+/- {:0.4f})".format(np.mean(results_lr['test_f1_micro']), np.std(results_lr['test_f1_micro'])))

print("CV Logloss: {:0.4f} (+/- {:0.4f})".format(np.mean(-1*results_lr['test_neg_log_loss']), np.std(-1*results_lr['test_neg_log_loss'])))

```
其他模型与以上代码结构相似，只需替换掉相应部分即可。得到最佳结果是Logistic Regression、Naive Bayes 和Multilayer Perceptron 达到的0.65-0.67左右的f1 score，其他如Random Forest、ExtraTree 等，都没有达到0.4。结果分数普遍较低可能是因为部分性格的数据量太少（数据量不均衡）导致的，而如果在模型学习之前进行under sampling的处理会减少很多数据量，结果反倒降低了准确率。

# 4. 马男角色性格预测

我从网上搜到了马男第一季前两集的剧本（只找到了前两集...），将BoJack、Todd、Diane、Mr. Peanutbutter 和Princess Carolyn 说过的话整理出来，喂进训练好的lr 模型中（f1 score: 0.67）进行预测,得到以下结果：     

| Caracter  | BoJack    | Todd  | Diane | Mr. Peanutbutter  | Princess Carolyn  |
| ---       | ---       | ---   | ---   | ---               | ---               |
| MBTI type | INTJ      | ESFP  | INTP  | ISTP              | ENTP              |

乍一看，除了Peanutbutter 的Introvert 很出人意料之外，其他也没有很明显不合理的地方...为了~~徒劳地~~验证结果正确性，我看了一会儿Reddit，找到很多人对马男角色性格特点的看法[[3]](#3)。结果发现大家对各个人物并没有统一意见（准确的说意见差距还是很大的），而我的模型预测结果也与一些人的意见相符。考虑到0.67 的f1 score，这算是一个比较满意的结果了...

# -1. 一些总结
* tutorial 和 user guide 十分重要
* 多实践，多分析，好好看基础

*参考资料*
1.  <span id="1"></span> https://www.kaggle.com/lbronchal/what-s-the-personality-of-kaggle-users/notebook
2.  <span id="2"></span> https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/
3.  <span id="3"></span> https://www.reddit.com/r/mbti/comments/4xfbv0/bojack_horseman_character_types/